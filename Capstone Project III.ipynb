{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using recurrent neural network in Keras to classify a movie review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Project, Sentiminent Analysis (NLP) is done using recurrent neural networks in Keras to try and classify a movie review as either positive or negative. The task will have the following steps:\n",
    "1. Get the dataset\n",
    "2. Preprocessing the Data\n",
    "3. Build the Model\n",
    "4. Train the model\n",
    "5. Test the Model\n",
    "6. Predict Something\n",
    "\n",
    "Keras will be used to import built-in IMDb movie review . This dataset contains a collection of 50,000 reviews from IMDb and contains an even number of positive and negative reviews. A negative review has a score ≤ 4 out of 10 (label = 0), and a positive review has a score ≥ 7 out of 10 (lable =1). Neutral reviews are not included in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Get The data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 2s 0us/step\n",
      "Data size: \n",
      "(50000,)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "# Setting the vocab size to download as working with a large data set and \n",
    "# words that aren't comonly used aren't necessary\n",
    "max_unique_words = 5000\n",
    "\n",
    "# Load data\n",
    "dataset = imdb.load_data(num_words=max_unique_words)\n",
    "(X_train, y_train), (X_test, y_test) = dataset\n",
    "X = np.concatenate((X_train, X_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "# data size\n",
    "print(\"Data size: \")\n",
    "print(X.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Review---\n",
      "[1, 2, 365, 1234, 5, 1156, 354, 11, 14, 2, 2, 7, 1016, 2, 2, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 2, 2, 1117, 1831, 2, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 2, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 2, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 2, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
      "---Label---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Look at data\n",
    "print('---Review---')\n",
    "print(X_train[6])\n",
    "print('---Label---')\n",
    "print(y_train[6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n",
      "---review with words---\n",
      "['the', 'and', 'full', 'involving', 'to', 'impressive', 'boring', 'this', 'as', 'and', 'and', 'br', 'villain', 'and', 'and', 'need', 'has', 'of', 'costumes', 'b', 'message', 'to', 'may', 'of', 'props', 'this', 'and', 'and', 'concept', 'issue', 'and', 'to', \"god's\", 'he', 'is', 'and', 'unfolds', 'movie', 'women', 'like', \"isn't\", 'surely', \"i'm\", 'and', 'to', 'toward', 'in', \"here's\", 'for', 'from', 'did', 'having', 'because', 'very', 'quality', 'it', 'is', 'and', 'and', 'really', 'book', 'is', 'both', 'too', 'worked', 'carl', 'of', 'and', 'br', 'of', 'reviewer', 'closer', 'figure', 'really', 'there', 'will', 'and', 'things', 'is', 'far', 'this', 'make', 'mistakes', 'and', 'was', \"couldn't\", 'of', 'few', 'br', 'of', 'you', 'to', \"don't\", 'female', 'than', 'place', 'she', 'to', 'was', 'between', 'that', 'nothing', 'and', 'movies', 'get', 'are', 'and', 'br', 'yes', 'female', 'just', 'its', 'because', 'many', 'br', 'of', 'overly', 'to', 'descent', 'people', 'time', 'very', 'bland']\n",
      "---label---\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Look at data by mapping back to words\n",
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print('---review with words---')\n",
    "print([id2word.get(i, ' ') for i in X_train[6]])\n",
    "print('---label---')\n",
    "print(y_train[6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Review length---\n",
      "Mean 500.00 words, standard dev of word length 0.00 in train sample\n",
      "Mean 500.00 words, standard dev of word length 0.00 in test sample\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    1,    2,  365, 1234,    5, 1156,  354,   11,\n",
       "         14,    2,    2,    7, 1016,    2,    2,  356,   44,    4, 1349,\n",
       "        500,  746,    5,  200,    4, 4132,   11,    2,    2, 1117, 1831,\n",
       "          2,    5, 4831,   26,    6,    2, 4183,   17,  369,   37,  215,\n",
       "       1345,  143,    2,    5, 1838,    8, 1974,   15,   36,  119,  257,\n",
       "         85,   52,  486,    9,    6,    2,    2,   63,  271,    6,  196,\n",
       "         96,  949, 4121,    4,    2,    7,    4, 2212, 2436,  819,   63,\n",
       "         47,   77,    2,  180,    6,  227,   11,   94, 2494,    2,   13,\n",
       "        423,    4,  168,    7,    4,   22,    5,   89,  665,   71,  270,\n",
       "         56,    5,   13,  197,   12,  161,    2,   99,   76,   23,    2,\n",
       "          7,  419,  665,   40,   91,   85,  108,    7,    4, 2084,    5,\n",
       "       4773,   81,   55,   52, 1901])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trimming length to 500 or padding if less than 500\n",
    "max_num_words = 500\n",
    "X_train = sequence.pad_sequences(X_train,\n",
    "                                 maxlen=max_num_words)\n",
    "X_test = sequence.pad_sequences(X_test,\n",
    "                                maxlen=max_num_words)\n",
    "\n",
    "# Look at review length to confirm all at 500\n",
    "print(\"---Review length---\")\n",
    "result = [len(x) for x in X_train]\n",
    "print(\"Mean %.2f words, standard dev of word length %.2f in train sample\" %\n",
    "      (np.mean(result), np.std(result)))\n",
    "result = [len(i) for i in X_test]\n",
    "print(\"Mean %.2f words, standard dev of word length %.2f in test sample\" %\n",
    "      (np.mean(result), np.std(result)))\n",
    "\n",
    "X_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size: \n",
      "(25000, 500)\n",
      "(25000,)\n",
      "(25000, 500)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "# Checking test and train data\n",
    "# data size\n",
    "print(\"Data size: \")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 32)           160032    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 242,593\n",
      "Trainable params: 242,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "model.add(Embedding(max_unique_words+1, 32, input_length=max_num_words))\n",
    "# LSTM layer\n",
    "model.add(LSTM(128))\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "250/250 [==============================] - 253s 1s/step - loss: 0.5020 - accuracy: 0.7472\n",
      "Epoch 2/5\n",
      "250/250 [==============================] - 270s 1s/step - loss: 0.3483 - accuracy: 0.8563\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 267s 1s/step - loss: 0.2595 - accuracy: 0.8977\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 265s 1s/step - loss: 0.2186 - accuracy: 0.9155\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 267s 1s/step - loss: 0.1974 - accuracy: 0.9254\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "\n",
    "model.fit(X_train, y_train, epochs=num_epochs, batch_size=batch_size)\n",
    "\n",
    "print('Training complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 103s 132ms/step - loss: 0.3165 - accuracy: 0.8754\n",
      "Test Loss: 0.31646719574928284\n",
      "Test Accuracy: 87.5440001487732 %\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {} %'.format(test_acc * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-30-1d50ab06f608>:25: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "label is [[1]], thus the review is positive. The prediction value is [[0.8957933]]\n"
     ]
    }
   ],
   "source": [
    "# predict on a sample text with padding.\n",
    "\n",
    "\n",
    "def sample_predict(sample_sentence):\n",
    "    \"\"\"\n",
    "    Function that first converts a sentence into tokens, with punctuation\n",
    "    and word case removed. It is then converted to the respective integer\n",
    "    using the imdb word to integer index. A prediction is then made to\n",
    "    determine if it is a positive or negative review.\n",
    "\n",
    "    \"\"\"\n",
    "    remove = dict.fromkeys(map(ord, '\\n ' + string.punctuation))\n",
    "    word_list = []\n",
    "    converted_tokens = []\n",
    "    \n",
    "    for word in sample_sentence.split():\n",
    "        word = word.translate(remove).lower()\n",
    "        word_list.append(word) \n",
    "    \n",
    "    for word in word_list:\n",
    "        converted_tokens.append(word2id[word])\n",
    "    \n",
    "    converted_tokens_padded =  sequence.pad_sequences([converted_tokens], maxlen=max_num_words)\n",
    "    predict = model.predict(converted_tokens_padded)\n",
    "    predict_class = model.predict_classes(converted_tokens_padded)\n",
    "\n",
    "    if predict_class == 1:\n",
    "        review_sentiment = f'label is {predict_class}, thus the review is positive. The prediction value is {predict}'\n",
    "    else:\n",
    "        review_sentiment = f'label is {predict_class}, thus the review is negative. The prediction value is {predict}'\n",
    "\n",
    "    return review_sentiment\n",
    "\n",
    "sample_sentence = ('The movie was impressive. I really liked the acting.')\n",
    "print(sample_predict(sample_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
